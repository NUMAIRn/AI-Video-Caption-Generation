{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMDff7A0IdnQPSka5exH986",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NUMAIRn/AI-Video-Caption-Generation/blob/main/AI_Generated_Captions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lx3U78XehZiR"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "from transformers import VisionEncoderDecoderModel, ViTImageProcessor, AutoTokenizer\n",
        "import torch\n",
        "from PIL import Image\n",
        "\n",
        "# Load your pre-trained model\n",
        "model = VisionEncoderDecoderModel.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\n",
        "feature_extractor = ViTImageProcessor.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# Settings for caption generation\n",
        "max_length = 16\n",
        "num_beams = 4\n",
        "gen_kwargs = {\"max_length\": max_length, \"num_beams\": num_beams}\n",
        "\n",
        "# Function to generate captions for frames\n",
        "def predict_step(images):\n",
        "    pixel_values = feature_extractor(images=images, return_tensors=\"pt\").pixel_values\n",
        "    pixel_values = pixel_values.to(device)\n",
        "    output_ids = model.generate(pixel_values, **gen_kwargs)\n",
        "    preds = tokenizer.batch_decode(output_ids, skip_special_tokens=True)\n",
        "    preds = [pred.strip() for pred in preds]\n",
        "    return preds\n",
        "\n",
        "# Function to overlay caption on a frame with text wrapping\n",
        "def overlay_caption_on_frame(frame, caption):\n",
        "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
        "    font_scale = 0.5  # Start with a small font scale\n",
        "    font_color = (255, 255, 255)\n",
        "    font_thickness = 2\n",
        "    background_color = (0, 0, 0)  # Black background for the text\n",
        "\n",
        "    frame_height, frame_width = frame.shape[:2]\n",
        "\n",
        "    # Split text into multiple lines if it is too long\n",
        "    wrapped_caption = []\n",
        "    max_width = frame_width - 40  # Allow some padding\n",
        "\n",
        "    words = caption.split(' ')\n",
        "    line = \"\"\n",
        "    for word in words:\n",
        "        test_line = line + word + \" \"\n",
        "        text_size = cv2.getTextSize(test_line, font, font_scale, font_thickness)[0]\n",
        "        if text_size[0] < max_width:\n",
        "            line = test_line\n",
        "        else:\n",
        "            wrapped_caption.append(line.strip())\n",
        "            line = word + \" \"\n",
        "    wrapped_caption.append(line.strip())\n",
        "\n",
        "    # Dynamically adjust font size based on frame size\n",
        "    while True:\n",
        "        text_size = cv2.getTextSize(wrapped_caption[0], font, font_scale, font_thickness)[0]\n",
        "        if text_size[0] <= max_width:\n",
        "            break\n",
        "        font_scale -= 0.1  # Decrease font size if text doesn't fit\n",
        "\n",
        "    # Positioning text: start from the bottom of the frame\n",
        "    y_offset = frame_height - 30 * len(wrapped_caption)\n",
        "\n",
        "    for line in wrapped_caption:\n",
        "        text_size = cv2.getTextSize(line, font, font_scale, font_thickness)[0]\n",
        "        text_x = (frame_width - text_size[0]) // 2\n",
        "        text_y = y_offset + text_size[1]\n",
        "\n",
        "        # Create a background rectangle for text visibility\n",
        "        cv2.rectangle(frame, (text_x - 10, text_y - text_size[1] - 10),\n",
        "                      (text_x + text_size[0] + 10, text_y + 10), background_color, cv2.FILLED)\n",
        "        cv2.putText(frame, line, (text_x, text_y), font, font_scale, font_color, font_thickness)\n",
        "\n",
        "        y_offset += text_size[1] + 15  # Move to the next line\n",
        "\n",
        "# Main function to process the video\n",
        "def process_video(video_path, output_path):\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
        "    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "\n",
        "    # Define the codec and create VideoWriter object for the output video\n",
        "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "    out = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\n",
        "\n",
        "    frame_count = 0\n",
        "    images_to_caption = []\n",
        "    captioned_frames_indices = []\n",
        "\n",
        "    # Process the video frame by frame to capture the first frame of each second\n",
        "    while cap.isOpened():\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        # Capture first frame of each second\n",
        "        if frame_count % fps == 0:\n",
        "            # Convert frame (numpy array) to PIL image\n",
        "            pil_image = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
        "            images_to_caption.append(pil_image)\n",
        "            captioned_frames_indices.append(frame_count)\n",
        "\n",
        "        frame_count += 1\n",
        "\n",
        "    # Generate captions for the selected frames\n",
        "    captions = predict_step(images_to_caption)\n",
        "\n",
        "    # Reset to process the video again and write each frame back with the appropriate caption\n",
        "    cap.set(cv2.CAP_PROP_POS_FRAMES, 0)\n",
        "    frame_count = 0\n",
        "    caption_index = 0\n",
        "\n",
        "    while cap.isOpened():\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        # Check if it's time to apply a new caption (every second)\n",
        "        if frame_count % fps == 0 and caption_index < len(captions):\n",
        "            current_caption = captions[caption_index]\n",
        "            caption_index += 1\n",
        "\n",
        "        # Apply the current caption to every frame in the second\n",
        "        overlay_caption_on_frame(frame, current_caption)\n",
        "\n",
        "        # Write the frame (with the current caption) to the output video\n",
        "        out.write(frame)\n",
        "        frame_count += 1\n",
        "\n",
        "    cap.release()\n",
        "    out.release()\n",
        "    cv2.destroyAllWindows()\n",
        "\n",
        "# Call the function\n",
        "process_video('input_video_filepath.mp4', 'output_video_with_captions.mp4')\n"
      ]
    }
  ]
}